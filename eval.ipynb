{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 04:35:22 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import re, argparse\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate.utils import fsdp_utils\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07683",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd15788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inter_answer(text: str) -> int:\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    return 0\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return extract_inter_answer(answer)\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "def extract_numerical_answer(answer_text):\n",
    "    # GSM8K answers end with #### followed by the numerical answer\n",
    "    match = re.search(r\"#### ([-\\d,]+)\", answer_text)\n",
    "    if match:\n",
    "        # Remove commas and convert to int\n",
    "        return int(match.group(1).replace(\",\", \"\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db94d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 04:35:28 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-24 04:35:28 [config.py:1472] Using max model len 256\n",
      "INFO 07-24 04:35:29 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-24 04:35:29 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-24 04:35:29 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870', speculative_config=None, tokenizer='/home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-24 04:35:33 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-24 04:35:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-24 04:35:33 [gpu_model_runner.py:1770] Starting to load model /home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870...\n",
      "INFO 07-24 04:35:33 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-24 04:35:33 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da30c8cebc8f4af4aea88972a430de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 04:35:37 [default_loader.py:272] Loading weights took 3.16 seconds\n",
      "INFO 07-24 04:35:37 [gpu_model_runner.py:1801] Model loading took 14.2488 GiB and 3.362404 seconds\n",
      "INFO 07-24 04:35:42 [backends.py:508] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/8304f5f314/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-24 04:35:42 [backends.py:519] Dynamo bytecode transform time: 5.24 s\n",
      "INFO 07-24 04:35:45 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 07-24 04:36:03 [backends.py:193] Compiling a graph for general shape takes 19.89 s\n",
      "INFO 07-24 04:36:10 [monitor.py:34] torch.compile takes 25.14 s in total\n",
      "INFO 07-24 04:36:11 [gpu_worker.py:232] Available KV cache memory: 112.75 GiB\n",
      "INFO 07-24 04:36:11 [kv_cache_utils.py:716] GPU KV cache size: 2,111,200 tokens\n",
      "INFO 07-24 04:36:11 [kv_cache_utils.py:720] Maximum concurrency for 256 tokens per request: 8246.88x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:18<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 04:36:30 [gpu_model_runner.py:2326] Graph capturing finished in 19 secs, took 0.65 GiB\n",
      "INFO 07-24 04:36:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 52.75 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=256,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    enforce_eager=False,  # Use Flash Attention 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853e5f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m eval_data = []\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdata\u001b[49m):\n\u001b[32m      3\u001b[39m     proccessed = {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: item[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: SYSTEM_PROMPT + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + item[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mother_answer\u001b[39m\u001b[33m\"\u001b[39m: extract_hash_answer(item[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m      9\u001b[39m     }\n\u001b[32m     10\u001b[39m     eval_data.append(proccessed)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eval_data = []\n",
    "for i, item in enumerate(data):\n",
    "    proccessed = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"prompt\": SYSTEM_PROMPT + \" \" + item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"numerical_answer\": extract_numerical_answer(item[\"answer\"]),\n",
    "        \"other_answer\": extract_hash_answer(item[\"answer\"]),\n",
    "    }\n",
    "    eval_data.append(proccessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [item[\"prompt\"] for item in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/11 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating:   0%|          | 0/11 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=786, stop=None)\n",
    "outputs = llm.generate(prompts, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f031a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  198, 65354,   304,   279,  2701,  3561,   510,    27, 19895,   287,\n",
       "           397,  9338,   522, 19895,   287,   397,    27,  9217,   397,  9338,\n",
       "           522,  9217,   397, 17599,   323,   220,    18,   315,   806,  4780,\n",
       "          1973,   220,    22, 87770,   369, 15786,    13,  8886, 22502,   374,\n",
       "          3931,  1119,   220,    23, 34254,    13,  1416, 17599,   323,   806,\n",
       "          4780,  1366,   311,  4332,   279, 87770, 18308,    11,  1246,  1657,\n",
       "         34254,   646,  1817,   315,  1105,   614,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    predicted_answer = extract_xml_answer(generated_text)\n",
    "    ground_truth = eval_data[i][\"numerical_answer\"]\n",
    "    #print(ground_truth, predicted_answer)\n",
    "    if int(predicted_answer) == int(ground_truth):\n",
    "        correct += 1\n",
    "    print(correct / (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_lengths = []\n",
    "for output in outputs:\n",
    "  # Get the first completion for the prompt\n",
    "  first_completion = output.outputs[0]\n",
    "  \n",
    "  # Get the number of tokens in this completion\n",
    "  num_tokens = len(first_completion.token_ids)\n",
    "  \n",
    "  # Add it to our list\n",
    "  response_lengths.append(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(response_lengths) / len(response_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
