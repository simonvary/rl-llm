{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb05472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 04:30:48 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import re, argparse\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate.utils import fsdp_utils\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f07683",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd15788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inter_answer(text: str) -> int:\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    return 0\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return extract_inter_answer(answer)\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "def extract_numerical_answer(answer_text):\n",
    "    # GSM8K answers end with #### followed by the numerical answer\n",
    "    match = re.search(r\"#### ([-\\d,]+)\", answer_text)\n",
    "    if match:\n",
    "        # Remove commas and convert to int\n",
    "        return int(match.group(1).replace(\",\", \"\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/ubuntu/alex/verifiers/outputs/Qwen/Qwen2.5-7B-Instruct-gsm8k-discount0.99999-seed42capacityblock1/checkpoint-1870'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=256,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    enforce_eager=False,  # Use Flash Attention 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c57d94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af59c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b853e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for i, item in enumerate(data):\n",
    "    proccessed = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"prompt\": SYSTEM_PROMPT + \" \" + item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"numerical_answer\": extract_numerical_answer(item[\"answer\"]),\n",
    "        \"other_answer\": extract_hash_answer(item[\"answer\"]),\n",
    "    }\n",
    "    eval_data.append(proccessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b27b3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [item[\"prompt\"] for item in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/11 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating:   0%|          | 0/11 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=786, stop=None)\n",
    "outputs = llm.generate(prompts, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f031a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  198, 65354,   304,   279,  2701,  3561,   510,    27, 19895,   287,\n",
       "           397,  9338,   522, 19895,   287,   397,    27,  9217,   397,  9338,\n",
       "           522,  9217,   397, 17599,   323,   220,    18,   315,   806,  4780,\n",
       "          1973,   220,    22, 87770,   369, 15786,    13,  8886, 22502,   374,\n",
       "          3931,  1119,   220,    23, 34254,    13,  1416, 17599,   323,   806,\n",
       "          4780,  1366,   311,  4332,   279, 87770, 18308,    11,  1246,  1657,\n",
       "         34254,   646,  1817,   315,  1105,   614,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, output in enumerate(outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    predicted_answer = extract_xml_answer(generated_text)\n",
    "    ground_truth = eval_data[i][\"numerical_answer\"]\n",
    "    #print(ground_truth, predicted_answer)\n",
    "    if int(predicted_answer) == int(ground_truth):\n",
    "        correct += 1\n",
    "    print(correct / (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_lengths = []\n",
    "for output in outputs:\n",
    "  # Get the first completion for the prompt\n",
    "  first_completion = output.outputs[0]\n",
    "  \n",
    "  # Get the number of tokens in this completion\n",
    "  num_tokens = len(first_completion.token_ids)\n",
    "  \n",
    "  # Add it to our list\n",
    "  response_lengths.append(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(response_lengths) / len(response_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
